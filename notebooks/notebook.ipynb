{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My name is XXXX XXXX this complaint is not mad...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I searched on XXXX for XXXXXXXX XXXX  and was ...</td>\n",
       "      <td>Fraud or scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have a particular account that is stating th...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have not supplied proof under the doctrine o...</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello i'm writing regarding account on my cred...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353427</th>\n",
       "      <td>Collections account I have no knowledge of</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353428</th>\n",
       "      <td>Dear CFPB Team, The reason for my complaint is...</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353429</th>\n",
       "      <td>FRCA violations : Failing to Follow Debt Dispu...</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353430</th>\n",
       "      <td>My Father, a XXXX XXXX  acquired an HECM rever...</td>\n",
       "      <td>Struggling to pay mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353431</th>\n",
       "      <td>I have tried to contact cash app about a fraud...</td>\n",
       "      <td>Fraud or scam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353432 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Consumer complaint narrative                                 Issue\n",
       "0       My name is XXXX XXXX this complaint is not mad...  Incorrect information on your report\n",
       "1       I searched on XXXX for XXXXXXXX XXXX  and was ...                         Fraud or scam\n",
       "2       I have a particular account that is stating th...  Incorrect information on your report\n",
       "3       I have not supplied proof under the doctrine o...     Attempts to collect debt not owed\n",
       "4       Hello i'm writing regarding account on my cred...  Incorrect information on your report\n",
       "...                                                   ...                                   ...\n",
       "353427         Collections account I have no knowledge of     Attempts to collect debt not owed\n",
       "353428  Dear CFPB Team, The reason for my complaint is...     Attempts to collect debt not owed\n",
       "353429  FRCA violations : Failing to Follow Debt Dispu...     Attempts to collect debt not owed\n",
       "353430  My Father, a XXXX XXXX  acquired an HECM rever...            Struggling to pay mortgage\n",
       "353431  I have tried to contact cash app about a fraud...                         Fraud or scam\n",
       "\n",
       "[353432 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_df = pd.read_csv(\"../data/complaints.csv\")\n",
    "complaints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect information on your report    229305\n",
      "Attempts to collect debt not owed        73163\n",
      "Communication tactics                    21243\n",
      "Struggling to pay mortgage               17374\n",
      "Fraud or scam                            12347\n",
      "Name: Issue, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "issue_counts = complaints_df[\"Issue\"].value_counts()\n",
    "print(issue_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.barh(issue_counts.index, issue_counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Issue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029.6670080807623\n"
     ]
    }
   ],
   "source": [
    "avg_length = complaints_df['Consumer complaint narrative'].str.len().mean()\n",
    "print(avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of complaint grouped by classification is:\n",
      " Issue\n",
      "Attempts to collect debt not owed       1000.366401\n",
      "Communication tactics                    751.250294\n",
      "Fraud or scam                           1360.773953\n",
      "Incorrect information on your report     982.515222\n",
      "Struggling to pay mortgage              1880.483711\n",
      "Name: Consumer complaint narrative, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_length_by_classification = complaints_df.groupby('Issue')['Consumer complaint narrative'].apply(lambda x: x.str.len().mean())\n",
    "print(\"The average length of complaint grouped by classification is:\\n\", avg_length_by_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total word count of complaints is: 64920688\n",
      "The total character count of complaints is: 363917270\n"
     ]
    }
   ],
   "source": [
    "word_count = complaints_df['Consumer complaint narrative'].str.split().apply(len).sum()\n",
    "char_count = complaints_df['Consumer complaint narrative'].str.len().sum()\n",
    "print(\"The total word count of complaints is:\", word_count)\n",
    "print(\"The total character count of complaints is:\", char_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\garre\\Documents\\NSS_Projects\\cfpb_text_classification-gangsta-s-paradise\\notebooks\\notebook.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m issue_df \u001b[39m=\u001b[39m complaints_df[complaints_df[\u001b[39m'\u001b[39m\u001b[39mIssue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m issue]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Remove stopwords and special characters from the text\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m issue_df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m issue_df[\u001b[39m'\u001b[39;49m\u001b[39mConsumer complaint narrative\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[^\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms]\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, x\u001b[39m.\u001b[39;49mlower()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m issue_df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m issue_df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords_list \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m word\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Get the most common words\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\garre\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\garre\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\garre\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1138\u001b[0m             values,\n\u001b[0;32m   1139\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1141\u001b[0m         )\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\garre\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\garre\\Documents\\NSS_Projects\\cfpb_text_classification-gangsta-s-paradise\\notebooks\\notebook.ipynb Cell 8\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m issue_df \u001b[39m=\u001b[39m complaints_df[complaints_df[\u001b[39m'\u001b[39m\u001b[39mIssue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m issue]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Remove stopwords and special characters from the text\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m issue_df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m issue_df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, x\u001b[39m.\u001b[39mlower()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m issue_df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m issue_df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords_list \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m word\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Get the most common words\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Get the list of stopwords\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "results_df = pd.DataFrame(columns=['Issue', 'Words', 'Frequency'])\n",
    "\n",
    "# Loop over each issue\n",
    "for issue in complaints_df['Issue'].unique():\n",
    "    # Filter the data to include only the current issue\n",
    "    issue_df = complaints_df[complaints_df['Issue'] == issue]\n",
    "    \n",
    "    # Remove stopwords and special characters from the text\n",
    "    issue_df['Consumer complaint narrative'] = issue_df['Consumer complaint narrative'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    issue_df['Consumer complaint narrative'] = issue_df['Consumer complaint narrative'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords_list and not word.startswith('x')))\n",
    "    \n",
    "    # Get the most common words\n",
    "    words = nltk.FreqDist(issue_df['Consumer complaint narrative'].str.cat(sep=' ').split())\n",
    "    top_words = pd.DataFrame(words.most_common(5), columns=['Words', 'Frequency'])\n",
    "    top_words['Issue'] = issue\n",
    "    results_df = results_df.append(top_words, ignore_index=True)\n",
    "    \n",
    "    # Get the most common bigrams\n",
    "    bigrams = nltk.FreqDist(list(nltk.bigrams(issue_df['Consumer complaint narrative'].str.cat(sep=' ').split())))\n",
    "    top_bigrams = pd.DataFrame(bigrams.most_common(5), columns=['Words', 'Frequency'])\n",
    "    top_bigrams['Words'] = top_bigrams['Words'].apply(lambda x: ' '.join(word for word in x if word != 'xxxx'))\n",
    "    top_bigrams['Words'] = top_bigrams['Words'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    top_bigrams['Issue'] = issue\n",
    "    results_df = results_df.append(top_bigrams, ignore_index=True)\n",
    "    \n",
    "    # Get the most common trigrams\n",
    "    trigrams = nltk.FreqDist(list(nltk.trigrams(issue_df['Consumer complaint narrative'].str.cat(sep=' ').split())))\n",
    "    top_trigrams = pd.DataFrame(trigrams.most_common(5), columns=['Words', 'Frequency'])\n",
    "    top_trigrams['Words'] = top_trigrams['Words'].apply(lambda x: ' '.join(word for word in x if word != 'xxxx'))\n",
    "    top_trigrams['Words'] = top_trigrams['Words'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    top_trigrams['Issue'] = issue\n",
    "    results_df = results_df.append(top_trigrams, ignore_index=True)\n",
    "\n",
    "#Display results_df\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for Issue, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(Issue))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(complaints['Consumer complaint narrative'], complaints['Issue'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize a string\n",
    "def tokenize_string(string):\n",
    "    return nltk.word_tokenize(string)\n",
    "\n",
    "# Apply the function to the 'strings' column\n",
    "complaints['complaint_tokens'] = complaints['Consumer complaint narrative'].apply(tokenize_string)\n",
    "\n",
    "# Output the result\n",
    "print(complaints['complaint_tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = complaints_df[['Consumer complaint narrative']]\n",
    "y = complaints_df['Issue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big G Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the RNN model\n",
    "sequence_length = 10\n",
    "num_features = X_train.shape[2]\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, num_features)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, testidation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse = model.etestuate(X_test, y_test)\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311966    Incorrect information on your report\n",
       "336446    Incorrect information on your report\n",
       "245715       Attempts to collect debt not owed\n",
       "151559    Incorrect information on your report\n",
       "114611    Incorrect information on your report\n",
       "                          ...                 \n",
       "119879       Attempts to collect debt not owed\n",
       "259178                   Communication tactics\n",
       "131932    Incorrect information on your report\n",
       "146867    Incorrect information on your report\n",
       "121958       Attempts to collect debt not owed\n",
       "Name: Issue, Length: 282745, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = complaints_df['Consumer complaint narrative']\n",
    "y = complaints_df['Issue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complaints_df['Consumer complaint narrative']\n",
    "y = complaints_df['Issue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, Sequential, optimizers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Create a tokenizer to vectorize text data\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "encoder = LabelBinarizer().fit(y_train)\n",
    "y_train = encoder.transform(y_train).ravel()\n",
    "\n",
    "encoder = LabelBinarizer().fit(y_test)\n",
    "y_test = encoder.transform(y_test).ravel()\n",
    "\n",
    "# Convert text data to sequences of integers\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure all sequences have the same length\n",
    "max_length = 200\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 200, 64)           4832832   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,866,181\n",
      "Trainable params: 4,866,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "num_features = len(tokenizer.word_index) + 1\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(num_features, 64, input_length=max_length))\n",
    "model.add(layers.LSTM(64))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(len(encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 282745\n  y sizes: 1413725\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\garre\\Documents\\NSS_Projects\\cfpb_text_classification-gangsta-s-paradise\\notebooks\\notebook.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/garre/Documents/NSS_Projects/cfpb_text_classification-gangsta-s-paradise/notebooks/notebook.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\garre\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\garre\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1846\u001b[0m         label,\n\u001b[0;32m   1847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1848\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1849\u001b[0m         ),\n\u001b[0;32m   1850\u001b[0m     )\n\u001b[0;32m   1851\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1852\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 282745\n  y sizes: 1413725\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Pretrained Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade keras-nlp tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install from terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digits Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "X = complaints_df['Consumer complaint narrative']\n",
    "y = complaints_df['Issue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the target labels as integers\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Define the input shape of your data\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Define your model architecture\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digits Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(complaints_df['Consumer complaint narrative'])\n",
    "\n",
    "# Sort the indices in the sparse matrix\n",
    "#X = tf.sparse.reorder(X)\n",
    "\n",
    "y = complaints_df['Issue']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "# Define the Keras model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to array or tensorflow sparsetensor (sparsereorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]}')\n",
    "print(f'Test accuracy: {score[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
